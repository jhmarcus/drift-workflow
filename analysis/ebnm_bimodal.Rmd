---
title: "Empircal Bayes Normal Means with Bimodal Priors"
author: "jhmarcus"
date: "2019-05-05"
output: workflowr::wflow_html
---

Here I explore the idea of "hacking" `ashr` to solve the Empirical Bayes Normal Means problem with a bimodal prior, specifically with the modes of the prior at 0 and 1. The idea is we'd like to "penalize" against estimating intermediate effects i.e. we shrink the effects to 1 if there large enough and 0 if their small enough, accounting for the precision of the estimate and learning the right level to shrink. 

# Imports

```{r warning=FALSE, message=FALSE}
library(ggplot2)
library(dplyr)
library(tidyr)
```

# Functions

Here are some helper function for simulation, fitting, and plotting.

```{r}
sim = function(n0, n1, sigma_e){
  n = n0+n1
  beta = c(rep(0, n0), rep(1, n1))
  s = abs(rnorm(n, 0, sigma_e))
  betahat = rnorm(n, beta, s)
  
  return(list(betahat=betahat, s=s, beta=beta, n=n))
}

fit = function(betahat, s, beta, m=20){

  b = seq(1.0, 0.0, length=m)
  a = seq(0.0, 1.0, length=m)
  bimodal_g = ashr:::unimix(rep(0, 2*m), c(rep(0, m),b), c(a, rep(1,m)))
  ash_res = ashr::ash(betahat, s, g=bimodal_g, fixg=FALSE, outputlevel=4)
  betapm = ash_res$flash_data$postmean
  df = data.frame(betahat=betahat, beta=beta, betapm=betapm, s=s, idx=1:length(betahat))  

  return(df)
  
}

plot_sim = function(df, title){
  
  gath_df = df %>% gather(variable, value, -idx, -s)
  p0 = ggplot(gath_df, aes(x=idx, y=value, 
                        color=factor(variable, levels=c("beta", "betahat", "betapm")))) + 
      geom_point() + 
      theme_bw() +
      labs(color="") +
      xlab("Variable") + 
      ylab("Value") +
      theme(legend.position="bottom")
  
  min_betahat = min(df$betahat)
  max_betahat = max(df$betahat)
  p1 = ggplot(df, aes(betahat, betapm, color=s)) + 
       geom_point() + viridis::scale_color_viridis() + 
       theme_bw() + 
       theme(legend.position="bottom") +
       xlim(c(min_betahat, max_betahat)) +
       ylim(c(min_betahat, max_betahat)) + 
       geom_abline() 

  p = cowplot::plot_grid(p0, p1, nrow=1) 
  title = cowplot::ggdraw() + cowplot::draw_label(title)
  print(cowplot::plot_grid(title, p, ncol=1, rel_heights=c(0.1, 1)))

}
```

# Simulations

I simulated a bunch of normal means scenarios where the true $\beta$s are set to 0 or 1. In each simulation I specify the number of zeros `n0` the number of ones `n1` and standard deviation used to simulate std. errors.

```{r}
n0 = c(rep(40, 3), rep(25, 3), rep(10, 3), rep(0, 3))
n1 = c(rep(40, 3), rep(55, 3), rep(70, 3), rep(80, 3))
sigma_e = rep(c(.05, .1, .25), 4)

for(i in 1:length(n0)){
  sim_res = sim(n0[i], n1[i], sigma_e[i])
  betahat = sim_res$betahat
  s = sim_res$s
  beta = sim_res$beta
  df = fit(betahat, s, beta, m=20)
  title = paste0("n0=",n0[i], ",n1=", n1[i], ",sigma_e=", sigma_e[i])
  plot_sim(df, title)
}
```

I think the idea roughly works! The most interesting scenarios to compare are when the std. errors of the estmates are high but the number of zeros and ones are different. Maybe we can define a term "bimodality" which I'm thinking is how bimodal the distribution is. When the bimodality is low (i.e. the prior distribution is closer to unimodal) the effects seem to be more correctly estimated. As we can see estimating more bimodal effects is a more difficult problem than unimodal effects. 

It would also be interesting to think more about how to weight the prior mixture proportions if that would be helpful.