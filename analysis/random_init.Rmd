---
title: "Random initialization"
author: "Jason Willwerscheid"
date: "7/6/2020"
output:
  workflowr::wflow_html:
    code_folding: hide
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, collapse = TRUE, comment = "#>",
                      fig.width = 5, fig.height = 4)
```

```{r load.pkgs}
suppressMessages({
  library(flashier)
  library(drift.alpha)
  library(tidyverse)
})
```


## Introduction

I use the four-population tree from a previous analysis:

```{r fig1, echo = FALSE, out.height = 400, out.width = 400}
knitr::include_graphics("assets/tree_4pop.png")
```

Here, I add an admixed population $E$ with admixture proportions of $1/2$ from population $B$, $1/3$ from population $C$, and $1/6$ from population $D$. So, individuals from population $A$ have data distributed 
$$N \left(\frac{1}{2}(a + b + e) + \frac{1}{3}(a + c + f) + \frac{1}{6}(a + c + g),\ \sigma_r^2 I_p \right)$$
I simulate data for 60 individuals per population.

```{r sim}
set.seed(666)

n_per_pop <- 60
p <- 10000

a <- rnorm(p)
b <- rnorm(p)
c <- rnorm(p)
d <- rnorm(p, sd = 0.5)
e <- rnorm(p, sd = 0.5)
f <- rnorm(p, sd = 0.5)
g <- rnorm(p, sd = 0.5)

popA <- c(rep(1, n_per_pop), rep(0, 4 * n_per_pop))
popB <- c(rep(0, n_per_pop), rep(1, n_per_pop), rep(0, 3 * n_per_pop))
popC <- c(rep(0, 2 * n_per_pop), rep(1, n_per_pop), rep(0, 2 * n_per_pop))
popD <- c(rep(0, 3 * n_per_pop), rep(1, n_per_pop), rep(0, n_per_pop))
popE <- c(rep(0, 4 * n_per_pop), rep(1, n_per_pop))

E.factor <- (a + b + e) / 2 + (a + c + f) / 3 + (a + c + g) / 6

Y <- cbind(popA, popB, popC, popD, popE) %*% 
  rbind(a + b + d, a + b + e, a + c + f, a + c + g, E.factor)
Y <- Y + rnorm(5 * n_per_pop * p, sd = 0.1)

plot_dr <- function(dr) {
  sd <- sqrt(dr$prior_s2)
  L <- dr$EL
  LDsqrt <- L %*% diag(sd)
  K <- ncol(LDsqrt)
  plot_loadings(LDsqrt[,1:K], rep(c("A", "B", "C", "D", "E"), each = n_per_pop)) +
    scale_color_brewer(palette="Set2")
}
```


## Greedy fit

```{r greedy}
greedy <- init_from_data(Y)
plot_dr(greedy)
```


## Optimal fit

If I initialize the loadings at their "true" values, I get the following fit:

```{r default_elbo}
dr_true <- init_from_EL(Y,
                        cbind(popA + popB + popC + popD,
                              popA + popB, popC + popD,
                              popA, popB, popC, popD),
                        cbind(a, b, c, d, e, f, g))
dr_true <- suppressWarnings({
  drift(dr_true, miniter = 2, verbose = FALSE, tol = 1e-4, maxiter = 2000)
})

plot_dr(dr_true)
```


## Default drift fit

Initializing at the greedy `flashier` fit and then running `drift` (using extrapolation with `beta.max` = 1) yields a much lower ELBO.

```{r default_drift}
options(extrapolate.control = list(beta.max = 1))
dr_default <- drift(greedy, tol = 1e-4, miniter = 2, maxiter = 2000, verbose = FALSE)

cat("Optimal ELBO (true factors):", dr_true$elbo,
    "\nDefault fit ELBO:           ", dr_default$elbo,
    "\nDifference:                 ", dr_true$elbo - dr_default$elbo, "\n")

plot_dr(dr_default)
```


## Random initialization

Next I run 10 trials with randomly initialized loadings. (The errors can safely be ignored.) I fix `K = 9` (the number of factors in the greedy fit) and keep the trial with the best ELBO after 100 iterations. I then run `drift` on this trial until convergence. The resulting ELBO is much better than the ELBO obtained using the default method, but the four population-specific factors get combined into two factors.

```{r rand_drift}
ntrials <- 10
elbo_vec <- rep(NA, ntrials)
rand_fit <- function(seed, K = 9, maxiter = 100, tol = 1e-4, verbose = FALSE) {
  set.seed(seed)
  EL <- matrix(runif(5 * n_per_pop * K), ncol = K)
  EL[, 1] <- 1
  EF <- t(solve(crossprod(EL), crossprod(EL, Y))) 
  suppressWarnings({
    dr <- drift(init_from_EL(Y, EL, EF), miniter = 20, maxiter = 20, 
                extrapolate = FALSE, verbose = verbose)
    dr <- drift(dr, miniter = 2, maxiter = maxiter, tol = tol, 
                extrapolate = TRUE, verbose = verbose)
  })
  return(dr)
}
best_elbo <- -Inf
for (i in 1:ntrials) {
  elbo_vec[i] <- -Inf
  try({
    # cat("TRIAL", i, "\n")
    dr <- rand_fit(i)
    # cat(" ELBO:", dr$elbo, "\n")
    elbo_vec[i] <- dr$elbo
    if (dr$elbo > best_elbo) {
      best_elbo <- dr$elbo
      best_dr <- dr
    }
  })
}
rand_dr <- drift(best_dr, maxiter = 2000, tol = 1e-4, verbose = FALSE)

cat("Optimal ELBO (true factors):", dr_true$elbo,
    "\nRandom initialization ELBO: ", rand_dr$elbo,
    "\nDifference:                 ", dr_true$elbo - rand_dr$elbo, "\n")

plot_dr(rand_dr)
```

In fact, all 10 trials yield better ELBOs than the default method:

```{r elbos}
elbo_df <- tibble(type = c("default", rep("random", 10)), 
                  elbo = c(dr_default$elbo, elbo_vec),
                  ind = 1:(ntrials + 1))
ggplot(elbo_df, aes(x = ind, y = elbo, col = type)) + geom_point()
```
